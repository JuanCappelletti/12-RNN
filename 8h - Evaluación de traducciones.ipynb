{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluacion de la traducción\n",
    "\n",
    "## Perplexity\n",
    "(Fuente: https://www.quora.com/How-does-perplexity-function-in-natural-language-processing)\n",
    "- Es una medida que indica con cuánta seguridad un modelo de lenguaje predecirá los datos de test (anotados).\n",
    "Por ejemplo, si a la salida del modelo de lenguaje esperamos la frase:\n",
    "\n",
    "\"I love NLP\"\n",
    "\n",
    "podemos estimar la seguridad del modelo como:\n",
    "\n",
    "$$ \\prod_{i=1}^N p(w_i) = p(\"NLP\" | \"I\", \"love\") * p(\"love\" | \"I\") * p(\"I\") $$\n",
    "\n",
    "Cuanto mas alto, mas seguro estará el modelo de su predicción.\n",
    "A medida que la cantidad de palabras en la oración crece, mas probabilidades se multiplican entre sí y este número tiende a mezclarse con la resolución de la máquina. Por ello es conveniente hacer la evaluación en un espacio logarítmico:\n",
    "\n",
    "$$ \\log_b \\prod_{i=1}^N p(w_i) = \\sum_{i=1}^N \\log_b p(w_i) $$\n",
    "\n",
    "Si normalizamos por la cantidad de palabras, nos independizamos del largo de la oración.  Si además multiplicamos por $-1$ nos queda:\n",
    "\n",
    "$$ ACE =  - \\frac{1}{N} \\sum_{i=1}^N \\log_b p(w_i) $$\n",
    "\n",
    "ACE: Average Cross Entropy\n",
    "\n",
    "Si el modelo de lenguaje está totalmente seguro de las palabras que está dando a su salida, entonces todas las probabilidades serán 1.  \n",
    "En el caso totalmente aleatorio, el modelo devuelve palabras con distribución uniforme, entonces todas las probabilidades serán $\\frac{1}{|V|}$, donde $|V|$ es el tamaño del vocabulario.\n",
    "En el peor de los casos, el modelo asigna una probabilidad cero a cada una de las palabras que esperamos a la salida.\n",
    "\n",
    "Se define la perplejidad (perplexity) como:\n",
    "\n",
    "$$PPL = b^{ACE} = b^{- \\frac{1}{N} \\sum_{i=1}^N \\log_b p(w_i)}$$\n",
    "\n",
    "Nótese que cuando el modelo está totalmente seguro, la PPL vale cero.  \n",
    "En el caso totalmente aleatorio, $p=\\frac{1}{|V|}$ y por lo tanto la PPL vale $|V|$.  \n",
    "En el peor de los casos, $p=0$ para todas las palabras y la PPL vale $\\infty$.\n",
    "\n",
    "En el caso de $b=e$ nos queda la categorical crossentropy calculada en Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación en Keras\n",
    "\n",
    "def perplexity(y_true, y_pred):\n",
    "    cross_entropy = K.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    perplexity = K.exp(cross_entropy)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bilingual Evaluation Understudy (BLEU Score)\n",
    "(Fuente: https://machinelearningmastery.com/calculate-bleu-score-for-text-python/)\n",
    "\n",
    "Sirve para evaluar una oración candidata contra un conjunto de oraciones de referencia.  \n",
    "\n",
    "### Precisión\n",
    "\n",
    "Candidata: This this this\n",
    "Referencia : This is a test\n",
    "\n",
    "$$ P=\\frac{\\#NgramaComunes}{\\#NgramasCandidata}$$\n",
    "\n",
    "Para $N=1$:  \n",
    "\n",
    "$$ P = \\frac{3}{3} = 1 $$\n",
    "\n",
    "Malas traducciones tienen una alta precisión. -> No sirve\n",
    "\n",
    "### Precisión modificada \n",
    "\n",
    "Se tiene en cuenta la cantidad de Ngramas en la oración de referencia, siendo este el límite a la hora de contabilizar las apariciones de la oración candidata.  \n",
    "Para el caso anterior:  \n",
    "$$PM = \\frac{2}{3} = 0.67 $$  \n",
    "\n",
    "Candidata: The the  \n",
    "Referencia: The cat is on the mat.\n",
    "\n",
    "$$ MP = \\frac{2}{2} $$\n",
    "\n",
    "\n",
    "Sigue teniendo problemas para medir la calidad de una traducción.\n",
    "Si las frases a comparar tienen una longitud muy distinta, no podemos afirmar que sean similares.  \n",
    "Si la frase candidata tiene mayor longitud que la de referencia, este aspecto se ve reflejado en la fórmula de precisión modificada anterior. Habrá muchos ngramas en la frase candidata que no aparecerán en la frase de referencia por lo que la precisión será menor.  \n",
    "Si la frase candidata es mas corta que la de referencia, debemos penalizar la precisión modificada por algún factor que sea menor que uno.\n",
    "\n",
    "\n",
    "### Bigrams, Trigrams, etc\n",
    "\n",
    "Candidata: The the cat -> \"The the\", \"the cat\"  \n",
    "Referencia: The cat is on the mat -> \"The cat\", \"cat is\", \"is on\", \"on the\", \"the mat\"  \n",
    "\n",
    "$$ P_2 = \\frac{1}{2} $$\n",
    "$$ MP_2 = \\frac{1}{2} $$\n",
    "\n",
    "### Penalización por brevedad (Brevity Penalty)\n",
    "\n",
    "$$ BP = \\begin{cases} 1 \\ \\mbox{si} \\ c > r \\\\ e ^{1-\\frac{r}{c}}\\end{cases} $$\n",
    "\n",
    "$$ MPBP= MP * BP $$ \n",
    "\n",
    "### BLEU Score\n",
    "\n",
    "Se calcula sobre un rango de Ngrams (Ej: Típicamente de 1 a 4) \n",
    "\n",
    "$$BLEU = PB \\exp\\left(\\sum_{n=1}^N w_n \\log \\text{MP}_n\\right)$$ \n",
    "\n",
    "$w_n$ es un peso que se le da a cada una de las MP de cada Ngram. Típicamente vale $\\frac{1}{N}$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071067811865476\n"
     ]
    }
   ],
   "source": [
    "# En NLTK:\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [[\"this\",\"is\",\"test\"],[\"this\",\"is\",\"a\",\"a\"]]\n",
    "candidate = [\"this\",\"is\",\"a\",\"test\",\"a\"]\n",
    "score = sentence_bleu(reference, candidate,weights=(0.5,0.5))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Score en corpus\n",
    "Para el caso de BLEU Score de un corpus completo se utiliza el micro average precision. El mismo consiste en sumar los casos satisfactorios de todas las oraciones y dividir por la suma de los tamaños de cada oración candidato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# two references for one document\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "references = [[['this', 'is', 'a', 'test'], ['this', 'is' 'test']]]\n",
    "candidates = [['this', 'is', 'a', 'test']]\n",
    "score = corpus_bleu(references, candidates,weights=(1,))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El BLEU Score en corpus es una medida que tiene muy alta correlación con los puntajes otorgados por humanos a las traducciones. Es por ello que es uno de los mas utilizados. No pasa lo mismo con el BLEU Score en oraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 (NLP)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
